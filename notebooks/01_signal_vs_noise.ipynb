{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9259693",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m      7\u001b[0m DROP_COLUMNS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsymmetrique Activity Index\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsymmetrique Profile Index\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA free copy of Mastering The Interview\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Nur existierende Spalten droppen (Sicherheitscheck)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m existing_drop \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m DROP_COLUMNS \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     18\u001b[0m df_reduced \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mexisting_drop)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemaining columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_reduced\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m DROP_COLUMNS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsymmetrique Activity Index\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsymmetrique Profile Index\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA free copy of Mastering The Interview\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Nur existierende Spalten droppen (Sicherheitscheck)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m existing_drop \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m DROP_COLUMNS \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     18\u001b[0m df_reduced \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mexisting_drop)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemaining columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_reduced\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 1) Entfernen der definierten Spalten\n",
    "# ============================================================\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"Asymmetrique Activity Index\",\n",
    "    \"Asymmetrique Profile Index\",\n",
    "    \"Asymmetrique Activity Score\",\n",
    "    \"Asymmetrique Profile Score\",\n",
    "    \"I agree to pay the amount through cheque\",\n",
    "    \"A free copy of Mastering The Interview\"\n",
    "]\n",
    "\n",
    "# Nur existierende Spalten droppen (Sicherheitscheck)\n",
    "existing_drop = [c for c in DROP_COLUMNS if c in df.columns]\n",
    "df_reduced = df.drop(columns=existing_drop).copy()\n",
    "\n",
    "print(\"Remaining columns:\", len(df_reduced.columns))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) NaNs in den ersten 3 Spalten prüfen\n",
    "# ============================================================\n",
    "\n",
    "first_three_cols = df_reduced.columns[:3]\n",
    "print(\"\\nFirst three columns:\", list(first_three_cols))\n",
    "\n",
    "for col in first_three_cols:\n",
    "    missing_count = df_reduced[col].isna().sum()\n",
    "    missing_pct = missing_count / len(df_reduced) * 100\n",
    "    unique_values = df_reduced[col].nunique(dropna=False)\n",
    "\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(\"Missing count:\", missing_count)\n",
    "    print(\"Missing %:\", round(missing_pct, 4))\n",
    "    print(\"Unique values:\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16e669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns under inspection: ['Lead Source', 'Do Not Email', 'Do Not Call']\n",
      "\n",
      "============================================================\n",
      "Column: Lead Source\n",
      "Missing count: 36\n",
      "Missing %: 0.3896\n",
      "Number of unique values (incl NaN): 22\n",
      "\n",
      "Top value counts:\n",
      "Lead Source\n",
      "Google              2868\n",
      "Direct Traffic      2543\n",
      "Olark Chat          1755\n",
      "Organic Search      1154\n",
      "Reference            534\n",
      "Welingak Website     142\n",
      "Referral Sites       125\n",
      "Facebook              55\n",
      "NaN                   36\n",
      "bing                   6\n",
      "google                 5\n",
      "Click2call             4\n",
      "Live Chat              2\n",
      "Social Media           2\n",
      "Press_Release          2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Column: Do Not Email\n",
      "Missing count: 0\n",
      "Missing %: 0.0\n",
      "Number of unique values (incl NaN): 2\n",
      "\n",
      "Top value counts:\n",
      "Do Not Email\n",
      "No     8506\n",
      "Yes     734\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "Column: Do Not Call\n",
      "Missing count: 0\n",
      "Missing %: 0.0\n",
      "Number of unique values (incl NaN): 2\n",
      "\n",
      "Top value counts:\n",
      "Do Not Call\n",
      "No     9238\n",
      "Yes       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols_to_check = df_reduced.columns[3:6]\n",
    "print(\"Columns under inspection:\", list(cols_to_check))\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Column: {col}\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_count = df_reduced[col].isna().sum()\n",
    "    missing_pct = missing_count / len(df_reduced) * 100\n",
    "    \n",
    "    print(\"Missing count:\", missing_count)\n",
    "    print(\"Missing %:\", round(missing_pct, 4))\n",
    "    \n",
    "    # Unique values\n",
    "    unique_values = df_reduced[col].nunique(dropna=False)\n",
    "    print(\"Number of unique values (incl NaN):\", unique_values)\n",
    "    \n",
    "    # Value counts (inkl NaN)\n",
    "    print(\"\\nTop value counts:\")\n",
    "    print(df_reduced[col].value_counts(dropna=False).head(15))\n",
    "    \n",
    "    # Check for empty strings\n",
    "    empty_string_count = (df_reduced[col] == \"\").sum()\n",
    "    if empty_string_count > 0:\n",
    "        print(\"Empty string count:\", empty_string_count)\n",
    "    \n",
    "    # Check for whitespace-only entries\n",
    "    whitespace_count = (df_reduced[col].astype(str).str.strip() == \"\").sum()\n",
    "    if whitespace_count > 0:\n",
    "        print(\"Whitespace-only entries:\", whitespace_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bee1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns under inspection: ['Converted', 'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit']\n",
      "\n",
      "======================================================================\n",
      "Column: Converted\n",
      "No NaNs found.\n",
      "\n",
      "Basic statistics:\n",
      "count    9240.000000\n",
      "mean        0.385390\n",
      "std         0.486714\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: Converted, dtype: float64\n",
      "\n",
      "Top 10 values:\n",
      "Converted\n",
      "0    5679\n",
      "1    3561\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "Column: TotalVisits\n",
      "NaNs found: 137\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prospect ID</th>\n",
       "      <th>Lead Number</th>\n",
       "      <th>Lead Origin</th>\n",
       "      <th>TotalVisits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>895d4905-f534-4f18-915b-8d239a72b5dc</td>\n",
       "      <td>659722</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3a0ce10f-d2c1-4213-a2bc-4f97bcd29699</td>\n",
       "      <td>659710</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>277ad6a6-4565-4a18-a1ff-e46e03f22663</td>\n",
       "      <td>659705</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>68f496c2-0073-470f-9c3c-7fb48f060ce5</td>\n",
       "      <td>659631</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>144807db-2895-4002-b52e-3eda79c22395</td>\n",
       "      <td>659283</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>63ebde80-a465-4cdc-ab5a-5e880a7138b0</td>\n",
       "      <td>659158</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0298b9a5-fedb-408b-a284-2d357583600f</td>\n",
       "      <td>659153</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3b74e995-4407-44de-9e59-622afb514261</td>\n",
       "      <td>658648</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1730b5e8-e435-41c6-9082-b9c98976bd16</td>\n",
       "      <td>658627</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>db2dc4b5-f603-4818-9b0c-0435923a4cd8</td>\n",
       "      <td>658623</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>d60b3133-9845-4d68-8675-6666c7dc57c9</td>\n",
       "      <td>658432</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>b6a07644-72d3-40b7-9c75-f996c6ee96a0</td>\n",
       "      <td>658258</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>b188ce42-4d14-4a53-80fb-e554ce3959ca</td>\n",
       "      <td>657611</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>35c83b23-921f-47da-8a95-5ebab14962ad</td>\n",
       "      <td>657590</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>149ea70e-6f74-4c23-8102-ff16b27d6b13</td>\n",
       "      <td>657247</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>aab15462-f9bd-4c1a-99f8-f1086cbbc9e0</td>\n",
       "      <td>656715</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>5befbbb7-2cf3-4a47-bb6f-086652eb131d</td>\n",
       "      <td>656563</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>5da64b7b-6302-45da-8462-872cc3cadd6a</td>\n",
       "      <td>656516</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>99bc24d5-cd9c-4f70-a4b7-b2c5d3da56d7</td>\n",
       "      <td>655352</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>6be73ad7-525d-11e6-96be-22000aa8e760</td>\n",
       "      <td>654173</td>\n",
       "      <td>Lead Import</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Prospect ID  Lead Number    Lead Origin  \\\n",
       "77   895d4905-f534-4f18-915b-8d239a72b5dc       659722  Lead Add Form   \n",
       "79   3a0ce10f-d2c1-4213-a2bc-4f97bcd29699       659710  Lead Add Form   \n",
       "81   277ad6a6-4565-4a18-a1ff-e46e03f22663       659705  Lead Add Form   \n",
       "88   68f496c2-0073-470f-9c3c-7fb48f060ce5       659631  Lead Add Form   \n",
       "120  144807db-2895-4002-b52e-3eda79c22395       659283  Lead Add Form   \n",
       "133  63ebde80-a465-4cdc-ab5a-5e880a7138b0       659158  Lead Add Form   \n",
       "134  0298b9a5-fedb-408b-a284-2d357583600f       659153  Lead Add Form   \n",
       "177  3b74e995-4407-44de-9e59-622afb514261       658648  Lead Add Form   \n",
       "179  1730b5e8-e435-41c6-9082-b9c98976bd16       658627  Lead Add Form   \n",
       "180  db2dc4b5-f603-4818-9b0c-0435923a4cd8       658623  Lead Add Form   \n",
       "195  d60b3133-9845-4d68-8675-6666c7dc57c9       658432  Lead Add Form   \n",
       "212  b6a07644-72d3-40b7-9c75-f996c6ee96a0       658258  Lead Add Form   \n",
       "267  b188ce42-4d14-4a53-80fb-e554ce3959ca       657611  Lead Add Form   \n",
       "269  35c83b23-921f-47da-8a95-5ebab14962ad       657590  Lead Add Form   \n",
       "299  149ea70e-6f74-4c23-8102-ff16b27d6b13       657247  Lead Add Form   \n",
       "348  aab15462-f9bd-4c1a-99f8-f1086cbbc9e0       656715  Lead Add Form   \n",
       "364  5befbbb7-2cf3-4a47-bb6f-086652eb131d       656563  Lead Add Form   \n",
       "369  5da64b7b-6302-45da-8462-872cc3cadd6a       656516  Lead Add Form   \n",
       "470  99bc24d5-cd9c-4f70-a4b7-b2c5d3da56d7       655352  Lead Add Form   \n",
       "585  6be73ad7-525d-11e6-96be-22000aa8e760       654173    Lead Import   \n",
       "\n",
       "     TotalVisits  \n",
       "77           NaN  \n",
       "79           NaN  \n",
       "81           NaN  \n",
       "88           NaN  \n",
       "120          NaN  \n",
       "133          NaN  \n",
       "134          NaN  \n",
       "177          NaN  \n",
       "179          NaN  \n",
       "180          NaN  \n",
       "195          NaN  \n",
       "212          NaN  \n",
       "267          NaN  \n",
       "269          NaN  \n",
       "299          NaN  \n",
       "348          NaN  \n",
       "364          NaN  \n",
       "369          NaN  \n",
       "470          NaN  \n",
       "585          NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic statistics:\n",
      "count    9103.000000\n",
      "mean        3.445238\n",
      "std         4.854853\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         3.000000\n",
      "75%         5.000000\n",
      "max       251.000000\n",
      "Name: TotalVisits, dtype: float64\n",
      "\n",
      "Top 10 values:\n",
      "TotalVisits\n",
      "0.0    2189\n",
      "2.0    1680\n",
      "3.0    1306\n",
      "4.0    1120\n",
      "5.0     783\n",
      "6.0     466\n",
      "1.0     395\n",
      "7.0     309\n",
      "8.0     224\n",
      "9.0     164\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "Column: Total Time Spent on Website\n",
      "No NaNs found.\n",
      "\n",
      "Basic statistics:\n",
      "count    9240.000000\n",
      "mean      487.698268\n",
      "std       548.021466\n",
      "min         0.000000\n",
      "25%        12.000000\n",
      "50%       248.000000\n",
      "75%       936.000000\n",
      "max      2272.000000\n",
      "Name: Total Time Spent on Website, dtype: float64\n",
      "\n",
      "Top 10 values:\n",
      "Total Time Spent on Website\n",
      "0      2193\n",
      "60       19\n",
      "74       18\n",
      "127      18\n",
      "75       18\n",
      "234      17\n",
      "62       17\n",
      "157      17\n",
      "87       17\n",
      "32       17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "Column: Page Views Per Visit\n",
      "NaNs found: 137\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prospect ID</th>\n",
       "      <th>Lead Number</th>\n",
       "      <th>Lead Origin</th>\n",
       "      <th>Page Views Per Visit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>895d4905-f534-4f18-915b-8d239a72b5dc</td>\n",
       "      <td>659722</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3a0ce10f-d2c1-4213-a2bc-4f97bcd29699</td>\n",
       "      <td>659710</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>277ad6a6-4565-4a18-a1ff-e46e03f22663</td>\n",
       "      <td>659705</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>68f496c2-0073-470f-9c3c-7fb48f060ce5</td>\n",
       "      <td>659631</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>144807db-2895-4002-b52e-3eda79c22395</td>\n",
       "      <td>659283</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>63ebde80-a465-4cdc-ab5a-5e880a7138b0</td>\n",
       "      <td>659158</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0298b9a5-fedb-408b-a284-2d357583600f</td>\n",
       "      <td>659153</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3b74e995-4407-44de-9e59-622afb514261</td>\n",
       "      <td>658648</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1730b5e8-e435-41c6-9082-b9c98976bd16</td>\n",
       "      <td>658627</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>db2dc4b5-f603-4818-9b0c-0435923a4cd8</td>\n",
       "      <td>658623</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>d60b3133-9845-4d68-8675-6666c7dc57c9</td>\n",
       "      <td>658432</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>b6a07644-72d3-40b7-9c75-f996c6ee96a0</td>\n",
       "      <td>658258</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>b188ce42-4d14-4a53-80fb-e554ce3959ca</td>\n",
       "      <td>657611</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>35c83b23-921f-47da-8a95-5ebab14962ad</td>\n",
       "      <td>657590</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>149ea70e-6f74-4c23-8102-ff16b27d6b13</td>\n",
       "      <td>657247</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>aab15462-f9bd-4c1a-99f8-f1086cbbc9e0</td>\n",
       "      <td>656715</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>5befbbb7-2cf3-4a47-bb6f-086652eb131d</td>\n",
       "      <td>656563</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>5da64b7b-6302-45da-8462-872cc3cadd6a</td>\n",
       "      <td>656516</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>99bc24d5-cd9c-4f70-a4b7-b2c5d3da56d7</td>\n",
       "      <td>655352</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>6be73ad7-525d-11e6-96be-22000aa8e760</td>\n",
       "      <td>654173</td>\n",
       "      <td>Lead Import</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Prospect ID  Lead Number    Lead Origin  \\\n",
       "77   895d4905-f534-4f18-915b-8d239a72b5dc       659722  Lead Add Form   \n",
       "79   3a0ce10f-d2c1-4213-a2bc-4f97bcd29699       659710  Lead Add Form   \n",
       "81   277ad6a6-4565-4a18-a1ff-e46e03f22663       659705  Lead Add Form   \n",
       "88   68f496c2-0073-470f-9c3c-7fb48f060ce5       659631  Lead Add Form   \n",
       "120  144807db-2895-4002-b52e-3eda79c22395       659283  Lead Add Form   \n",
       "133  63ebde80-a465-4cdc-ab5a-5e880a7138b0       659158  Lead Add Form   \n",
       "134  0298b9a5-fedb-408b-a284-2d357583600f       659153  Lead Add Form   \n",
       "177  3b74e995-4407-44de-9e59-622afb514261       658648  Lead Add Form   \n",
       "179  1730b5e8-e435-41c6-9082-b9c98976bd16       658627  Lead Add Form   \n",
       "180  db2dc4b5-f603-4818-9b0c-0435923a4cd8       658623  Lead Add Form   \n",
       "195  d60b3133-9845-4d68-8675-6666c7dc57c9       658432  Lead Add Form   \n",
       "212  b6a07644-72d3-40b7-9c75-f996c6ee96a0       658258  Lead Add Form   \n",
       "267  b188ce42-4d14-4a53-80fb-e554ce3959ca       657611  Lead Add Form   \n",
       "269  35c83b23-921f-47da-8a95-5ebab14962ad       657590  Lead Add Form   \n",
       "299  149ea70e-6f74-4c23-8102-ff16b27d6b13       657247  Lead Add Form   \n",
       "348  aab15462-f9bd-4c1a-99f8-f1086cbbc9e0       656715  Lead Add Form   \n",
       "364  5befbbb7-2cf3-4a47-bb6f-086652eb131d       656563  Lead Add Form   \n",
       "369  5da64b7b-6302-45da-8462-872cc3cadd6a       656516  Lead Add Form   \n",
       "470  99bc24d5-cd9c-4f70-a4b7-b2c5d3da56d7       655352  Lead Add Form   \n",
       "585  6be73ad7-525d-11e6-96be-22000aa8e760       654173    Lead Import   \n",
       "\n",
       "     Page Views Per Visit  \n",
       "77                    NaN  \n",
       "79                    NaN  \n",
       "81                    NaN  \n",
       "88                    NaN  \n",
       "120                   NaN  \n",
       "133                   NaN  \n",
       "134                   NaN  \n",
       "177                   NaN  \n",
       "179                   NaN  \n",
       "180                   NaN  \n",
       "195                   NaN  \n",
       "212                   NaN  \n",
       "267                   NaN  \n",
       "269                   NaN  \n",
       "299                   NaN  \n",
       "348                   NaN  \n",
       "364                   NaN  \n",
       "369                   NaN  \n",
       "470                   NaN  \n",
       "585                   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic statistics:\n",
      "count    9103.000000\n",
      "mean        2.362820\n",
      "std         2.161418\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         3.000000\n",
      "max        55.000000\n",
      "Name: Page Views Per Visit, dtype: float64\n",
      "\n",
      "Top 10 values:\n",
      "Page Views Per Visit\n",
      "0.0    2189\n",
      "2.0    1795\n",
      "3.0    1196\n",
      "4.0     896\n",
      "1.0     651\n",
      "5.0     517\n",
      "1.5     306\n",
      "6.0     244\n",
      "2.5     241\n",
      "NaN     137\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols_to_check = df_reduced.columns[6:10]\n",
    "print(\"Columns under inspection:\", list(cols_to_check))\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Column: {col}\")\n",
    "    \n",
    "    # --- Missing check ---\n",
    "    nan_mask = df_reduced[col].isna()\n",
    "    n_nans = nan_mask.sum()\n",
    "    \n",
    "    if n_nans == 0:\n",
    "        print(\"No NaNs found.\")\n",
    "    else:\n",
    "        print(f\"NaNs found: {n_nans}\")\n",
    "        display(\n",
    "            df_reduced.loc[nan_mask,\n",
    "                           [\"Prospect ID\", \"Lead Number\", \"Lead Origin\", col]]\n",
    "            .head(20)\n",
    "        )\n",
    "    \n",
    "    # --- Basic stats for numeric columns ---\n",
    "    if pd.api.types.is_numeric_dtype(df_reduced[col]):\n",
    "        print(\"\\nBasic statistics:\")\n",
    "        print(df_reduced[col].describe())\n",
    "        \n",
    "        print(\"\\nTop 10 values:\")\n",
    "        print(df_reduced[col].value_counts(dropna=False).head(10))\n",
    "    \n",
    "    # --- Unique values for non-numeric ---\n",
    "    else:\n",
    "        print(\"\\nValue counts:\")\n",
    "        print(df_reduced[col].value_counts(dropna=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANALYSIS: TotalVisits Missing Values\n",
      "================================================================================\n",
      "\n",
      "Total rows: 9240\n",
      "Missing TotalVisits: 137\n",
      "Percentage missing: 1.4827 %\n",
      "\n",
      "Lead Origin distribution for missing TotalVisits:\n",
      "------------------------------------------------------------\n",
      "Lead Origin\n",
      "Lead Add Form     110\n",
      "Lead Import        24\n",
      "API                 2\n",
      "Quick Add Form      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Conversion comparison:\n",
      "------------------------------------------------------------\n",
      "Overall conversion rate: 0.3854\n",
      "Conversion (Missing TotalVisits): 0.7299\n",
      "Conversion (Not Missing TotalVisits): 0.3802\n",
      "\n",
      "Sample rows with missing TotalVisits:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prospect ID</th>\n",
       "      <th>Lead Number</th>\n",
       "      <th>Lead Origin</th>\n",
       "      <th>TotalVisits</th>\n",
       "      <th>Page Views Per Visit</th>\n",
       "      <th>Converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>895d4905-f534-4f18-915b-8d239a72b5dc</td>\n",
       "      <td>659722</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3a0ce10f-d2c1-4213-a2bc-4f97bcd29699</td>\n",
       "      <td>659710</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>277ad6a6-4565-4a18-a1ff-e46e03f22663</td>\n",
       "      <td>659705</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>68f496c2-0073-470f-9c3c-7fb48f060ce5</td>\n",
       "      <td>659631</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>144807db-2895-4002-b52e-3eda79c22395</td>\n",
       "      <td>659283</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>63ebde80-a465-4cdc-ab5a-5e880a7138b0</td>\n",
       "      <td>659158</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0298b9a5-fedb-408b-a284-2d357583600f</td>\n",
       "      <td>659153</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3b74e995-4407-44de-9e59-622afb514261</td>\n",
       "      <td>658648</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1730b5e8-e435-41c6-9082-b9c98976bd16</td>\n",
       "      <td>658627</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>db2dc4b5-f603-4818-9b0c-0435923a4cd8</td>\n",
       "      <td>658623</td>\n",
       "      <td>Lead Add Form</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Prospect ID  Lead Number    Lead Origin  \\\n",
       "77   895d4905-f534-4f18-915b-8d239a72b5dc       659722  Lead Add Form   \n",
       "79   3a0ce10f-d2c1-4213-a2bc-4f97bcd29699       659710  Lead Add Form   \n",
       "81   277ad6a6-4565-4a18-a1ff-e46e03f22663       659705  Lead Add Form   \n",
       "88   68f496c2-0073-470f-9c3c-7fb48f060ce5       659631  Lead Add Form   \n",
       "120  144807db-2895-4002-b52e-3eda79c22395       659283  Lead Add Form   \n",
       "133  63ebde80-a465-4cdc-ab5a-5e880a7138b0       659158  Lead Add Form   \n",
       "134  0298b9a5-fedb-408b-a284-2d357583600f       659153  Lead Add Form   \n",
       "177  3b74e995-4407-44de-9e59-622afb514261       658648  Lead Add Form   \n",
       "179  1730b5e8-e435-41c6-9082-b9c98976bd16       658627  Lead Add Form   \n",
       "180  db2dc4b5-f603-4818-9b0c-0435923a4cd8       658623  Lead Add Form   \n",
       "\n",
       "     TotalVisits  Page Views Per Visit  Converted  \n",
       "77           NaN                   NaN          1  \n",
       "79           NaN                   NaN          1  \n",
       "81           NaN                   NaN          1  \n",
       "88           NaN                   NaN          1  \n",
       "120          NaN                   NaN          1  \n",
       "133          NaN                   NaN          1  \n",
       "134          NaN                   NaN          1  \n",
       "177          NaN                   NaN          1  \n",
       "179          NaN                   NaN          1  \n",
       "180          NaN                   NaN          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis block complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS: TotalVisits Missing Values\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Maske für Missing\n",
    "missing_mask = df_reduced[\"TotalVisits\"].isna()\n",
    "\n",
    "n_missing = missing_mask.sum()\n",
    "n_total = len(df_reduced)\n",
    "\n",
    "print(f\"\\nTotal rows: {n_total}\")\n",
    "print(f\"Missing TotalVisits: {n_missing}\")\n",
    "print(f\"Percentage missing: {round(n_missing / n_total * 100, 4)} %\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ In welchen Lead Origins treten sie auf?\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nLead Origin distribution for missing TotalVisits:\")\n",
    "print(\"-\"*60)\n",
    "print(df_reduced.loc[missing_mask, \"Lead Origin\"].value_counts())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Conversion Rate Vergleich\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nConversion comparison:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "conversion_missing = df_reduced.loc[missing_mask, \"Converted\"].mean()\n",
    "conversion_not_missing = df_reduced.loc[~missing_mask, \"Converted\"].mean()\n",
    "conversion_overall = df_reduced[\"Converted\"].mean()\n",
    "\n",
    "print(f\"Overall conversion rate: {round(conversion_overall, 4)}\")\n",
    "print(f\"Conversion (Missing TotalVisits): {round(conversion_missing, 4)}\")\n",
    "print(f\"Conversion (Not Missing TotalVisits): {round(conversion_not_missing, 4)}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Beispielhafte Zeilen anzeigen\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nSample rows with missing TotalVisits:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "display(\n",
    "    df_reduced.loc[missing_mask,\n",
    "                   [\"Prospect ID\", \"Lead Number\", \"Lead Origin\", \n",
    "                    \"TotalVisits\", \"Page Views Per Visit\", \"Converted\"]]\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis block complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0244832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANALYSIS: Conversion Rate by Lead Origin\n",
      "================================================================================\n",
      "\n",
      "Overall Conversion Rate: 0.3854\n",
      "\n",
      "Conversion Rate by Lead Origin:\n",
      "------------------------------------------------------------\n",
      "                         count      mean\n",
      "Lead Origin                             \n",
      "Quick Add Form               1  1.000000\n",
      "Lead Add Form              718  0.924791\n",
      "Landing Page Submission   4886  0.361850\n",
      "API                       3580  0.311453\n",
      "Lead Import                 55  0.236364\n",
      "\n",
      "Detailed Interpretation:\n",
      "------------------------------------------------------------\n",
      "Quick Add Form:\n",
      "  Number of Leads: 1.0\n",
      "  Conversion Rate: 1.0\n",
      "\n",
      "Lead Add Form:\n",
      "  Number of Leads: 718.0\n",
      "  Conversion Rate: 0.9248\n",
      "\n",
      "Landing Page Submission:\n",
      "  Number of Leads: 4886.0\n",
      "  Conversion Rate: 0.3619\n",
      "\n",
      "API:\n",
      "  Number of Leads: 3580.0\n",
      "  Conversion Rate: 0.3115\n",
      "\n",
      "Lead Import:\n",
      "  Number of Leads: 55.0\n",
      "  Conversion Rate: 0.2364\n",
      "\n",
      "================================================================================\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ANALYSIS: Conversion Rate by Lead Origin\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Gesamt-Conversion\n",
    "overall_conversion = df_reduced[\"Converted\"].mean()\n",
    "print(f\"\\nOverall Conversion Rate: {round(overall_conversion, 4)}\")\n",
    "\n",
    "print(\"\\nConversion Rate by Lead Origin:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "origin_summary = (\n",
    "    df_reduced\n",
    "    .groupby(\"Lead Origin\")[\"Converted\"]\n",
    "    .agg([\"count\", \"mean\"])\n",
    "    .sort_values(\"mean\", ascending=False)\n",
    ")\n",
    "\n",
    "print(origin_summary)\n",
    "\n",
    "print(\"\\nDetailed Interpretation:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for origin, row in origin_summary.iterrows():\n",
    "    print(f\"{origin}:\")\n",
    "    print(f\"  Number of Leads: {row['count']}\")\n",
    "    print(f\"  Conversion Rate: {round(row['mean'], 4)}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95130ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns under inspection: ['Last Activity', 'Country', 'Specialization', 'How did you hear about X Education', 'What is your current occupation', 'What matters most to you in choosing a course']\n",
      "\n",
      "================================================================================\n",
      "Column: Last Activity\n",
      "NaN count: 103\n",
      "NaN %: 1.1147\n",
      "Unique values (incl NaN): 18\n",
      "\n",
      "Top value counts:\n",
      "Last Activity\n",
      "Email Opened                    3437\n",
      "SMS Sent                        2745\n",
      "Olark Chat Conversation          973\n",
      "Page Visited on Website          640\n",
      "Converted to Lead                428\n",
      "Email Bounced                    326\n",
      "Email Link Clicked               267\n",
      "Form Submitted on Website        116\n",
      "NaN                              103\n",
      "Unreachable                       93\n",
      "Unsubscribed                      61\n",
      "Had a Phone Conversation          30\n",
      "Approached upfront                 9\n",
      "View in browser link Clicked       6\n",
      "Email Marked Spam                  2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Column: Country\n",
      "NaN count: 2461\n",
      "NaN %: 26.6342\n",
      "Unique values (incl NaN): 39\n",
      "\n",
      "Top value counts:\n",
      "Country\n",
      "India                   6492\n",
      "NaN                     2461\n",
      "United States             69\n",
      "United Arab Emirates      53\n",
      "Singapore                 24\n",
      "Saudi Arabia              21\n",
      "United Kingdom            15\n",
      "Australia                 13\n",
      "Qatar                     10\n",
      "Hong Kong                  7\n",
      "Bahrain                    7\n",
      "Oman                       6\n",
      "France                     6\n",
      "unknown                    5\n",
      "Kuwait                     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Column: Specialization\n",
      "NaN count: 1438\n",
      "NaN %: 15.5628\n",
      "Unique values (incl NaN): 20\n",
      "\n",
      "Top value counts:\n",
      "Specialization\n",
      "Select                               1942\n",
      "NaN                                  1438\n",
      "Finance Management                    976\n",
      "Human Resource Management             848\n",
      "Marketing Management                  838\n",
      "Operations Management                 503\n",
      "Business Administration               403\n",
      "IT Projects Management                366\n",
      "Supply Chain Management               349\n",
      "Banking, Investment And Insurance     338\n",
      "Media and Advertising                 203\n",
      "Travel and Tourism                    203\n",
      "International Business                178\n",
      "Healthcare Management                 159\n",
      "Hospitality Management                114\n",
      "Name: count, dtype: int64\n",
      "'Select' occurrences: 1942\n",
      "\n",
      "================================================================================\n",
      "Column: How did you hear about X Education\n",
      "NaN count: 2207\n",
      "NaN %: 23.8853\n",
      "Unique values (incl NaN): 11\n",
      "\n",
      "Top value counts:\n",
      "How did you hear about X Education\n",
      "Select                   5043\n",
      "NaN                      2207\n",
      "Online Search             808\n",
      "Word Of Mouth             348\n",
      "Student of SomeSchool     310\n",
      "Other                     186\n",
      "Multiple Sources          152\n",
      "Advertisements             70\n",
      "Social Media               67\n",
      "Email                      26\n",
      "SMS                        23\n",
      "Name: count, dtype: int64\n",
      "'Select' occurrences: 5043\n",
      "\n",
      "================================================================================\n",
      "Column: What is your current occupation\n",
      "NaN count: 2690\n",
      "NaN %: 29.1126\n",
      "Unique values (incl NaN): 7\n",
      "\n",
      "Top value counts:\n",
      "What is your current occupation\n",
      "Unemployed              5600\n",
      "NaN                     2690\n",
      "Working Professional     706\n",
      "Student                  210\n",
      "Other                     16\n",
      "Housewife                 10\n",
      "Businessman                8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Column: What matters most to you in choosing a course\n",
      "NaN count: 2709\n",
      "NaN %: 29.3182\n",
      "Unique values (incl NaN): 4\n",
      "\n",
      "Top value counts:\n",
      "What matters most to you in choosing a course\n",
      "Better Career Prospects      6528\n",
      "NaN                          2709\n",
      "Flexibility & Convenience       2\n",
      "Other                           1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols_to_check = df_reduced.columns[10:16]\n",
    "print(\"Columns under inspection:\", list(cols_to_check))\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Column: {col}\")\n",
    "    \n",
    "    # ---- Missing ----\n",
    "    nan_count = df_reduced[col].isna().sum()\n",
    "    nan_pct = nan_count / len(df_reduced) * 100\n",
    "    \n",
    "    print(f\"NaN count: {nan_count}\")\n",
    "    print(f\"NaN %: {round(nan_pct, 4)}\")\n",
    "    \n",
    "    # ---- Unique values ----\n",
    "    unique_vals = df_reduced[col].nunique(dropna=False)\n",
    "    print(f\"Unique values (incl NaN): {unique_vals}\")\n",
    "    \n",
    "    # ---- Top values ----\n",
    "    print(\"\\nTop value counts:\")\n",
    "    print(df_reduced[col].value_counts(dropna=False).head(15))\n",
    "    \n",
    "    # ---- Pseudo-Missing Checks ----\n",
    "    if df_reduced[col].dtype == \"object\":\n",
    "        select_count = df_reduced[col].astype(str).str.contains(\"Select\", case=False, na=False).sum()\n",
    "        not_provided_count = df_reduced[col].astype(str).str.contains(\"Not Provided\", case=False, na=False).sum()\n",
    "        empty_count = (df_reduced[col] == \"\").sum()\n",
    "        \n",
    "        if select_count > 0:\n",
    "            print(f\"'Select' occurrences: {select_count}\")\n",
    "        if not_provided_count > 0:\n",
    "            print(f\"'Not Provided' occurrences: {not_provided_count}\")\n",
    "        if empty_count > 0:\n",
    "            print(f\"Empty string occurrences: {empty_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns under inspection: ['Search', 'Magazine', 'Newspaper Article', 'X Education Forums', 'Newspaper', 'Digital Advertisement']\n",
      "\n",
      "================================================================================\n",
      "Column: Search\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 2\n",
      "\n",
      "Value counts:\n",
      "Search\n",
      "No     9226\n",
      "Yes      14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique raw labels:\n",
      "['No' 'Yes']\n",
      "\n",
      "================================================================================\n",
      "Column: Magazine\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 1\n",
      "\n",
      "Value counts:\n",
      "Magazine\n",
      "No    9240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique raw labels:\n",
      "['No']\n",
      "\n",
      "================================================================================\n",
      "Column: Newspaper Article\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 2\n",
      "\n",
      "Value counts:\n",
      "Newspaper Article\n",
      "No     9238\n",
      "Yes       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique raw labels:\n",
      "['No' 'Yes']\n",
      "\n",
      "================================================================================\n",
      "Column: X Education Forums\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 2\n",
      "\n",
      "Value counts:\n",
      "X Education Forums\n",
      "No     9239\n",
      "Yes       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique raw labels:\n",
      "['No' 'Yes']\n",
      "\n",
      "================================================================================\n",
      "Column: Newspaper\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 2\n",
      "\n",
      "Value counts:\n",
      "Newspaper\n",
      "No     9239\n",
      "Yes       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique raw labels:\n",
      "['No' 'Yes']\n",
      "\n",
      "================================================================================\n",
      "Column: Digital Advertisement\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 2\n",
      "\n",
      "Value counts:\n",
      "Digital Advertisement\n",
      "No     9236\n",
      "Yes       4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique raw labels:\n",
      "['No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "cols_to_check = df_reduced.columns[16:22]\n",
    "print(\"Columns under inspection:\", list(cols_to_check))\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Column: {col}\")\n",
    "    \n",
    "    # ---- Missing ----\n",
    "    nan_count = df_reduced[col].isna().sum()\n",
    "    nan_pct = nan_count / len(df_reduced) * 100\n",
    "    \n",
    "    print(f\"NaN count: {nan_count}\")\n",
    "    print(f\"NaN %: {round(nan_pct, 4)}\")\n",
    "    \n",
    "    # ---- Unique values ----\n",
    "    unique_vals = df_reduced[col].nunique(dropna=False)\n",
    "    print(f\"Unique values (incl NaN): {unique_vals}\")\n",
    "    \n",
    "    # ---- Value distribution ----\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(df_reduced[col].value_counts(dropna=False))\n",
    "    \n",
    "    # ---- Check for inconsistent labels ----\n",
    "    if df_reduced[col].dtype == \"object\":\n",
    "        print(\"\\nUnique raw labels:\")\n",
    "        print(df_reduced[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eef49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns under inspection: ['Through Recommendations', 'Receive More Updates About Our Courses', 'Tags', 'Lead Quality', 'Update me on Supply Chain Content', 'Get updates on DM Content']\n",
      "\n",
      "================================================================================\n",
      "Column: Through Recommendations\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 2\n",
      "\n",
      "Value counts:\n",
      "Through Recommendations\n",
      "No     9233\n",
      "Yes       7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Column: Receive More Updates About Our Courses\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 1\n",
      "\n",
      "Value counts:\n",
      "Receive More Updates About Our Courses\n",
      "No    9240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Column: Tags\n",
      "NaN count: 3353\n",
      "NaN %: 36.2879\n",
      "Unique values (incl NaN): 27\n",
      "\n",
      "Value counts:\n",
      "Tags\n",
      "NaN                                                  3353\n",
      "Will revert after reading the email                  2072\n",
      "Ringing                                              1203\n",
      "Interested in other courses                           513\n",
      "Already a student                                     465\n",
      "Closed by Horizzon                                    358\n",
      "switched off                                          240\n",
      "Busy                                                  186\n",
      "Lost to EINS                                          175\n",
      "Not doing further education                           145\n",
      "Interested  in full time MBA                          117\n",
      "Graduation in progress                                111\n",
      "invalid number                                         83\n",
      "Diploma holder (Not Eligible)                          63\n",
      "wrong number given                                     47\n",
      "opp hangup                                             33\n",
      "number not provided                                    27\n",
      "in touch with EINS                                     12\n",
      "Lost to Others                                          7\n",
      "Want to take admission but has financial problems       6\n",
      "Name: count, dtype: int64\n",
      "'Not Provided' occurrences: 27\n",
      "\n",
      "================================================================================\n",
      "Column: Lead Quality\n",
      "NaN count: 4767\n",
      "NaN %: 51.5909\n",
      "Unique values (incl NaN): 6\n",
      "\n",
      "Value counts:\n",
      "Lead Quality\n",
      "NaN                  4767\n",
      "Might be             1560\n",
      "Not Sure             1092\n",
      "High in Relevance     637\n",
      "Worst                 601\n",
      "Low in Relevance      583\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Column: Update me on Supply Chain Content\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 1\n",
      "\n",
      "Value counts:\n",
      "Update me on Supply Chain Content\n",
      "No    9240\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "Column: Get updates on DM Content\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 1\n",
      "\n",
      "Value counts:\n",
      "Get updates on DM Content\n",
      "No    9240\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cols_to_check = df_reduced.columns[22:28]\n",
    "print(\"Columns under inspection:\", list(cols_to_check))\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Column: {col}\")\n",
    "    \n",
    "    # ---- Missing ----\n",
    "    nan_count = df_reduced[col].isna().sum()\n",
    "    nan_pct = nan_count / len(df_reduced) * 100\n",
    "    \n",
    "    print(f\"NaN count: {nan_count}\")\n",
    "    print(f\"NaN %: {round(nan_pct, 4)}\")\n",
    "    \n",
    "    # ---- Unique values ----\n",
    "    unique_vals = df_reduced[col].nunique(dropna=False)\n",
    "    print(f\"Unique values (incl NaN): {unique_vals}\")\n",
    "    \n",
    "    # ---- Value distribution ----\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(df_reduced[col].value_counts(dropna=False).head(20))\n",
    "    \n",
    "    # ---- Check for pseudo-missing ----\n",
    "    if df_reduced[col].dtype == \"object\":\n",
    "        select_count = df_reduced[col].astype(str).str.contains(\"Select\", case=False, na=False).sum()\n",
    "        not_provided_count = df_reduced[col].astype(str).str.contains(\"Not Provided\", case=False, na=False).sum()\n",
    "        \n",
    "        if select_count > 0:\n",
    "            print(f\"'Select' occurrences: {select_count}\")\n",
    "        if not_provided_count > 0:\n",
    "            print(f\"'Not Provided' occurrences: {not_provided_count}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns under inspection: ['Lead Profile', 'City', 'Last Notable Activity']\n",
      "\n",
      "================================================================================\n",
      "Column: Lead Profile\n",
      "NaN count: 2709\n",
      "NaN %: 29.3182\n",
      "Unique values (incl NaN): 7\n",
      "\n",
      "Top value counts:\n",
      "Lead Profile\n",
      "Select                         4146\n",
      "NaN                            2709\n",
      "Potential Lead                 1613\n",
      "Other Leads                     487\n",
      "Student of SomeSchool           241\n",
      "Lateral Student                  24\n",
      "Dual Specialization Student      20\n",
      "Name: count, dtype: int64\n",
      "'Select' occurrences: 4146\n",
      "\n",
      "================================================================================\n",
      "Column: City\n",
      "NaN count: 1420\n",
      "NaN %: 15.368\n",
      "Unique values (incl NaN): 8\n",
      "\n",
      "Top value counts:\n",
      "City\n",
      "Mumbai                         3222\n",
      "Select                         2249\n",
      "NaN                            1420\n",
      "Thane & Outskirts               752\n",
      "Other Cities                    686\n",
      "Other Cities of Maharashtra     457\n",
      "Other Metro Cities              380\n",
      "Tier II Cities                   74\n",
      "Name: count, dtype: int64\n",
      "'Select' occurrences: 2249\n",
      "\n",
      "================================================================================\n",
      "Column: Last Notable Activity\n",
      "NaN count: 0\n",
      "NaN %: 0.0\n",
      "Unique values (incl NaN): 16\n",
      "\n",
      "Top value counts:\n",
      "Last Notable Activity\n",
      "Modified                        3407\n",
      "Email Opened                    2827\n",
      "SMS Sent                        2172\n",
      "Page Visited on Website          318\n",
      "Olark Chat Conversation          183\n",
      "Email Link Clicked               173\n",
      "Email Bounced                     60\n",
      "Unsubscribed                      47\n",
      "Unreachable                       32\n",
      "Had a Phone Conversation          14\n",
      "Email Marked Spam                  2\n",
      "Approached upfront                 1\n",
      "Resubscribed to emails             1\n",
      "View in browser link Clicked       1\n",
      "Form Submitted on Website          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "start_index = 28  # letzte geprüfte Position anpassen falls nötig\n",
    "cols_to_check = df_reduced.columns[start_index:start_index+6]\n",
    "\n",
    "print(\"Columns under inspection:\", list(cols_to_check))\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Column: {col}\")\n",
    "    \n",
    "    # ---- Missing ----\n",
    "    nan_count = df_reduced[col].isna().sum()\n",
    "    nan_pct = nan_count / len(df_reduced) * 100\n",
    "    \n",
    "    print(f\"NaN count: {nan_count}\")\n",
    "    print(f\"NaN %: {round(nan_pct, 4)}\")\n",
    "    \n",
    "    # ---- Unique values ----\n",
    "    unique_vals = df_reduced[col].nunique(dropna=False)\n",
    "    print(f\"Unique values (incl NaN): {unique_vals}\")\n",
    "    \n",
    "    # ---- Value distribution ----\n",
    "    print(\"\\nTop value counts:\")\n",
    "    print(df_reduced[col].value_counts(dropna=False).head(15))\n",
    "    \n",
    "    # ---- Pseudo-Missing ----\n",
    "    if df_reduced[col].dtype == \"object\":\n",
    "        select_count = df_reduced[col].astype(str).str.contains(\"Select\", case=False, na=False).sum()\n",
    "        not_provided_count = df_reduced[col].astype(str).str.contains(\"Not Provided\", case=False, na=False).sum()\n",
    "        \n",
    "        if select_count > 0:\n",
    "            print(f\"'Select' occurrences: {select_count}\")\n",
    "        if not_provided_count > 0:\n",
    "            print(f\"'Not Provided' occurrences: {not_provided_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0abcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLEANING: Converting pseudo-missing values to NaN\n",
      "================================================================================\n",
      "Pseudo-missing values replaced with NaN.\n",
      "\n",
      "Updated missing summary (top 15 columns):\n",
      "How did you hear about X Education               7250\n",
      "Lead Profile                                     6855\n",
      "Lead Quality                                     4767\n",
      "City                                             3669\n",
      "Specialization                                   3380\n",
      "Tags                                             3353\n",
      "What matters most to you in choosing a course    2709\n",
      "What is your current occupation                  2690\n",
      "Country                                          2461\n",
      "TotalVisits                                       137\n",
      "Page Views Per Visit                              137\n",
      "Last Activity                                     103\n",
      "Lead Source                                        36\n",
      "Converted                                           0\n",
      "Do Not Call                                         0\n",
      "dtype: int64\n",
      "\n",
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CLEANING: Converting pseudo-missing values to NaN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_clean = df_reduced.copy()\n",
    "\n",
    "# Define pseudo-missing tokens\n",
    "PSEUDO_TOKENS = [\"Select\", \"Not Provided\", \"\", \" \"]\n",
    "\n",
    "# Replace across entire dataframe\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype == \"object\":\n",
    "        df_clean[col] = df_clean[col].replace(PSEUDO_TOKENS, pd.NA)\n",
    "\n",
    "print(\"Pseudo-missing values replaced with NaN.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Check how many NaNs we now have per column\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "missing_summary = (\n",
    "    df_clean.isna()\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nUpdated missing summary (top 15 columns):\")\n",
    "print(missing_summary.head(15))\n",
    "\n",
    "print(\"\\nCleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc423b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Creating tracking vs non-tracking datasets\n",
      "================================================================================\n",
      "Total rows: 9240\n",
      "Rows WITHOUT tracking (NaN TotalVisits): 137\n",
      "Rows WITH tracking: 9103\n",
      "\n",
      "Conversion comparison:\n",
      "No tracking conversion: 0.7299\n",
      "With tracking conversion: 0.3802\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Creating tracking vs non-tracking datasets\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tracking_missing_mask = df_clean[\"TotalVisits\"].isna()\n",
    "\n",
    "df_no_tracking = df_clean.loc[tracking_missing_mask].copy()\n",
    "df_with_tracking = df_clean.loc[~tracking_missing_mask].copy()\n",
    "\n",
    "print(f\"Total rows: {len(df_clean)}\")\n",
    "print(f\"Rows WITHOUT tracking (NaN TotalVisits): {len(df_no_tracking)}\")\n",
    "print(f\"Rows WITH tracking: {len(df_with_tracking)}\")\n",
    "\n",
    "print(\"\\nConversion comparison:\")\n",
    "print(f\"No tracking conversion: {round(df_no_tracking['Converted'].mean(), 4)}\")\n",
    "print(f\"With tracking conversion: {round(df_with_tracking['Converted'].mean(), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030e505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datasets saved.\n"
     ]
    }
   ],
   "source": [
    "df_clean.to_csv(\"../data/processed/lead_scoring_cleaned.csv\", index=False)\n",
    "df_with_tracking.to_csv(\"../data/processed/lead_scoring_with_tracking.csv\", index=False)\n",
    "df_no_tracking.to_csv(\"../data/processed/lead_scoring_no_tracking.csv\", index=False)\n",
    "\n",
    "print(\"Processed datasets saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761bb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Olark Chat', 'Organic Search', 'Direct Traffic', 'Google',\n",
       "       'Referral Sites', 'Welingak Website', 'Reference', 'google',\n",
       "       'Facebook', nan, 'blog', 'Pay per Click Ads', 'bing',\n",
       "       'Social Media', 'WeLearn', 'Click2call', 'Live Chat',\n",
       "       'welearnblog_Home', 'youtubechannel', 'testone', 'Press_Release',\n",
       "       'NC_EDM'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[\"Lead Source\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64429440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "FINAL DATA AUDIT\n",
      "====================================================================================================\n",
      "\n",
      "1) DATASET SHAPE\n",
      "--------------------------------------------------\n",
      "Rows: 9240\n",
      "Columns: 31\n",
      "\n",
      "2) MISSING SUMMARY (Top 15)\n",
      "--------------------------------------------------\n",
      "How did you hear about X Education               7250\n",
      "Lead Profile                                     6855\n",
      "Lead Quality                                     4767\n",
      "City                                             3669\n",
      "Specialization                                   3380\n",
      "Tags                                             3353\n",
      "What matters most to you in choosing a course    2709\n",
      "What is your current occupation                  2690\n",
      "Country                                          2466\n",
      "TotalVisits                                       137\n",
      "Page Views Per Visit                              137\n",
      "Last Activity                                     103\n",
      "Lead Source                                        36\n",
      "Converted                                           0\n",
      "Do Not Call                                         0\n",
      "dtype: int64\n",
      "\n",
      "3) DATA TYPES\n",
      "--------------------------------------------------\n",
      "Prospect ID                                       object\n",
      "Lead Number                                        int64\n",
      "Lead Origin                                       object\n",
      "Lead Source                                       object\n",
      "Do Not Email                                      object\n",
      "Do Not Call                                       object\n",
      "Converted                                          int64\n",
      "TotalVisits                                      float64\n",
      "Total Time Spent on Website                        int64\n",
      "Page Views Per Visit                             float64\n",
      "Last Activity                                     object\n",
      "Country                                           object\n",
      "Specialization                                    object\n",
      "How did you hear about X Education                object\n",
      "What is your current occupation                   object\n",
      "What matters most to you in choosing a course     object\n",
      "Search                                            object\n",
      "Magazine                                          object\n",
      "Newspaper Article                                 object\n",
      "X Education Forums                                object\n",
      "Newspaper                                         object\n",
      "Digital Advertisement                             object\n",
      "Through Recommendations                           object\n",
      "Receive More Updates About Our Courses            object\n",
      "Tags                                              object\n",
      "Lead Quality                                      object\n",
      "Update me on Supply Chain Content                 object\n",
      "Get updates on DM Content                         object\n",
      "Lead Profile                                      object\n",
      "City                                              object\n",
      "Last Notable Activity                             object\n",
      "dtype: object\n",
      "\n",
      "4) NUMERIC SUMMARY\n",
      "--------------------------------------------------\n",
      "         Lead Number    Converted  TotalVisits  Total Time Spent on Website  \\\n",
      "count    9240.000000  9240.000000  9103.000000                  9240.000000   \n",
      "mean   617188.435606     0.385390     3.445238                   487.698268   \n",
      "std     23405.995698     0.486714     4.854853                   548.021466   \n",
      "min    579533.000000     0.000000     0.000000                     0.000000   \n",
      "25%    596484.500000     0.000000     1.000000                    12.000000   \n",
      "50%    615479.000000     0.000000     3.000000                   248.000000   \n",
      "75%    637387.250000     1.000000     5.000000                   936.000000   \n",
      "max    660737.000000     1.000000   251.000000                  2272.000000   \n",
      "\n",
      "       Page Views Per Visit  \n",
      "count           9103.000000  \n",
      "mean               2.362820  \n",
      "std                2.161418  \n",
      "min                0.000000  \n",
      "25%                1.000000  \n",
      "50%                2.000000  \n",
      "75%                3.000000  \n",
      "max               55.000000  \n",
      "\n",
      "5) TARGET DISTRIBUTION\n",
      "--------------------------------------------------\n",
      "Converted\n",
      "0    5679\n",
      "1    3561\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target ratio:\n",
      "Converted\n",
      "0    0.61461\n",
      "1    0.38539\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "6) NEGATIVE VALUE CHECK\n",
      "--------------------------------------------------\n",
      "Lead Number: OK (no negatives)\n",
      "Converted: OK (no negatives)\n",
      "TotalVisits: OK (no negatives)\n",
      "Total Time Spent on Website: OK (no negatives)\n",
      "Page Views Per Visit: OK (no negatives)\n",
      "\n",
      "7) LEAD SOURCE UNIQUE VALUES\n",
      "--------------------------------------------------\n",
      "['Olark Chat' 'Organic Search' 'Direct Traffic' 'Google' 'Referral Sites'\n",
      " 'Welingak Website' 'Reference' 'Facebook' nan 'Blog' 'Pay Per Click Ads'\n",
      " 'Bing' 'Social Media' 'Welearn' 'Click2Call' 'Live Chat'\n",
      " 'Welearnblog_Home' 'Youtubechannel' 'Testone' 'Press_Release' 'Nc_Edm']\n",
      "\n",
      "AUDIT COMPLETE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "df_clean[\"Lead Source\"] = df_clean[\"Lead Source\"].str.strip().str.title()\n",
    "df_clean[\"Country\"] = df_clean[\"Country\"].replace(\"unknown\", pd.NA)\n",
    "df_clean[[\"TotalVisits\", \n",
    "          \"Total Time Spent on Website\", \n",
    "          \"Page Views Per Visit\"]].describe()\n",
    "df_clean.dtypes\n",
    "df_clean[\"Converted\"].value_counts(normalize=True)\n",
    "print(\"=\"*100)\n",
    "print(\"FINAL DATA AUDIT\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Shape\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n1) DATASET SHAPE\")\n",
    "print(\"-\"*50)\n",
    "print(\"Rows:\", df_clean.shape[0])\n",
    "print(\"Columns:\", df_clean.shape[1])\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Missing Overview\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n2) MISSING SUMMARY (Top 15)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "missing_summary = (\n",
    "    df_clean.isna()\n",
    "    .sum()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(missing_summary.head(15))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Data Types\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n3) DATA TYPES\")\n",
    "print(\"-\"*50)\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Numeric Summary\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n4) NUMERIC SUMMARY\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "numeric_cols = df_clean.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "print(df_clean[numeric_cols].describe())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Target Distribution\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n5) TARGET DISTRIBUTION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(df_clean[\"Converted\"].value_counts())\n",
    "print(\"\\nTarget ratio:\")\n",
    "print(df_clean[\"Converted\"].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Check for negative values in numeric columns\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n6) NEGATIVE VALUE CHECK\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if (df_clean[col] < 0).any():\n",
    "        print(f\"Negative values found in {col}\")\n",
    "    else:\n",
    "        print(f\"{col}: OK (no negatives)\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ Lead Source normalization check\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n7) LEAD SOURCE UNIQUE VALUES\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(df_clean[\"Lead Source\"].unique())\n",
    "\n",
    "\n",
    "print(\"\\nAUDIT COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fe50ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Overwriting processed dataset\n",
      "================================================================================\n",
      "File overwritten at: ../data/processed/lead_scoring_cleaned.csv\n",
      "Overwrite complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Overwriting processed dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_path = \"../data/processed/lead_scoring_cleaned.csv\"\n",
    "\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"File overwritten at: {output_path}\")\n",
    "print(\"Overwrite complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b96fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9240, 31)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../data/processed/lead_scoring_cleaned.csv\")\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "DATA TYPE INSPECTION\n",
      "====================================================================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Prospect ID\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 9240\n",
      "Sample Values: ['7927b2df-8bba-4d29-b9a2-b6e0beafe620', '2a272436-5132-4136-86fa-dcc88c88f482', '8cc8c611-a219-4f35-ad23-fdfd2656bd8a', '0cc2df48-7cf4-4e39-9de9-19797f9b38cc', '3256f628-e534-4826-9d63-4a8b88782852']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Lead Number\n",
      "Data Type: int64\n",
      "Unique Values (incl NaN): 9240\n",
      "Min: 579533\n",
      "Max: 660737\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Lead Origin\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 5\n",
      "Sample Values: ['API', 'API', 'Landing Page Submission', 'Landing Page Submission', 'Landing Page Submission']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Lead Source\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 21\n",
      "Sample Values: ['Olark Chat', 'Organic Search', 'Direct Traffic', 'Direct Traffic', 'Google']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Do Not Email\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 2\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Do Not Call\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 2\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Converted\n",
      "Data Type: int64\n",
      "Unique Values (incl NaN): 2\n",
      "Min: 0\n",
      "Max: 1\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: TotalVisits\n",
      "Data Type: float64\n",
      "Unique Values (incl NaN): 42\n",
      "Min: 0.0\n",
      "Max: 251.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Total Time Spent on Website\n",
      "Data Type: int64\n",
      "Unique Values (incl NaN): 1731\n",
      "Min: 0\n",
      "Max: 2272\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Page Views Per Visit\n",
      "Data Type: float64\n",
      "Unique Values (incl NaN): 115\n",
      "Min: 0.0\n",
      "Max: 55.0\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Last Activity\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 18\n",
      "Sample Values: ['Page Visited on Website', 'Email Opened', 'Email Opened', 'Unreachable', 'Converted to Lead']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Country\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 39\n",
      "Sample Values: ['India', 'India', 'India', 'India', 'India']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Specialization\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 20\n",
      "Sample Values: ['Business Administration', 'Media and Advertising', 'Supply Chain Management', 'IT Projects Management', 'Finance Management']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: How did you hear about X Education\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 11\n",
      "Sample Values: ['Word Of Mouth', 'Other', 'Online Search', 'Word Of Mouth', 'Multiple Sources']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: What is your current occupation\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 7\n",
      "Sample Values: ['Unemployed', 'Unemployed', 'Student', 'Unemployed', 'Unemployed']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: What matters most to you in choosing a course\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 4\n",
      "Sample Values: ['Better Career Prospects', 'Better Career Prospects', 'Better Career Prospects', 'Better Career Prospects', 'Better Career Prospects']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Search\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 2\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Magazine\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 1\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Newspaper Article\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 2\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: X Education Forums\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 2\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Newspaper\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 2\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Digital Advertisement\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 2\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Through Recommendations\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 2\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Receive More Updates About Our Courses\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 1\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Tags\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 27\n",
      "Sample Values: ['Interested in other courses', 'Ringing', 'Will revert after reading the email', 'Ringing', 'Will revert after reading the email']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Lead Quality\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 6\n",
      "Sample Values: ['Low in Relevance', 'Might be', 'Not Sure', 'Might be', 'Low in Relevance']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Update me on Supply Chain Content\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 1\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Get updates on DM Content\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 1\n",
      "Sample Values: ['No', 'No', 'No', 'No', 'No']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Lead Profile\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 7\n",
      "Sample Values: ['Potential Lead', 'Potential Lead', 'Potential Lead', 'Potential Lead', 'Potential Lead']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: City\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 8\n",
      "Sample Values: ['Mumbai', 'Mumbai', 'Mumbai', 'Mumbai', 'Thane & Outskirts']\n",
      "\n",
      "------------------------------------------------------------\n",
      "Column: Last Notable Activity\n",
      "Data Type: object\n",
      "Unique Values (incl NaN): 16\n",
      "Sample Values: ['Modified', 'Email Opened', 'Email Opened', 'Modified', 'Modified']\n",
      "\n",
      "DATA TYPE CHECK COMPLETE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"DATA TYPE INSPECTION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for col in df_clean.columns:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Data Type: {df_clean[col].dtype}\")\n",
    "    \n",
    "    unique_count = df_clean[col].nunique(dropna=False)\n",
    "    print(f\"Unique Values (incl NaN): {unique_count}\")\n",
    "    \n",
    "    # Check for numeric-looking object columns\n",
    "    if df_clean[col].dtype == \"object\":\n",
    "        sample_vals = df_clean[col].dropna().astype(str).head(5).tolist()\n",
    "        print(\"Sample Values:\", sample_vals)\n",
    "    \n",
    "    # Check for suspicious numeric columns\n",
    "    if df_clean[col].dtype in [\"int64\", \"float64\"]:\n",
    "        print(\"Min:\", df_clean[col].min())\n",
    "        print(\"Max:\", df_clean[col].max())\n",
    "\n",
    "print(\"\\nDATA TYPE CHECK COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ff5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "REMOVING LEAKAGE + ID COLUMNS\n",
      "====================================================================================================\n",
      "\n",
      "Columns to remove:\n",
      "- Prospect ID\n",
      "- Lead Number\n",
      "- Lead Quality\n",
      "- Tags\n",
      "- Lead Profile\n",
      "- Last Notable Activity\n",
      "\n",
      "Columns removed successfully.\n",
      "\n",
      "New dataset shape:\n",
      "Rows: 9240\n",
      "Columns: 25\n",
      "\n",
      "Remaining columns:\n",
      "['Lead Origin', 'Lead Source', 'Do Not Email', 'Do Not Call', 'Converted', 'TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit', 'Last Activity', 'Country', 'Specialization', 'How did you hear about X Education', 'What is your current occupation', 'What matters most to you in choosing a course', 'Search', 'Magazine', 'Newspaper Article', 'X Education Forums', 'Newspaper', 'Digital Advertisement', 'Through Recommendations', 'Receive More Updates About Our Courses', 'Update me on Supply Chain Content', 'Get updates on DM Content', 'City']\n",
      "\n",
      "LEAKAGE REMOVAL COMPLETE\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"REMOVING LEAKAGE + ID COLUMNS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Define columns to remove\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"Prospect ID\",\n",
    "    \"Lead Number\",\n",
    "    \"Lead Quality\",\n",
    "    \"Tags\",\n",
    "    \"Lead Profile\",\n",
    "    \"Last Notable Activity\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Safety check (drop only if exists)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "existing_drop = [col for col in DROP_COLUMNS if col in df_clean.columns]\n",
    "\n",
    "print(\"\\nColumns to remove:\")\n",
    "for col in existing_drop:\n",
    "    print(\"-\", col)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Drop columns\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df_model = df_clean.drop(columns=existing_drop).copy()\n",
    "\n",
    "print(\"\\nColumns removed successfully.\")\n",
    "print(\"\\nNew dataset shape:\")\n",
    "print(\"Rows:\", df_model.shape[0])\n",
    "print(\"Columns:\", df_model.shape[1])\n",
    "\n",
    "print(\"\\nRemaining columns:\")\n",
    "print(df_model.columns.tolist())\n",
    "\n",
    "print(\"\\nLEAKAGE REMOVAL COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8f6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "LOGISTIC REGRESSION PIPELINE START\n",
      "====================================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 1️⃣ Copy dataset\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf_model\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 2️⃣ Separate target\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     24\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverted\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"LOGISTIC REGRESSION PIPELINE START\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Copy dataset\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df = df_model.copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Separate target\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "y = df[\"Converted\"]\n",
    "X = df.drop(columns=[\"Converted\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Identify column types\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nCategorical columns:\", len(categorical_cols))\n",
    "print(categorical_cols)\n",
    "\n",
    "print(\"\\nNumeric columns:\", len(numeric_cols))\n",
    "print(numeric_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Fill missing for categorical columns only\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].fillna(\"Missing\")\n",
    "\n",
    "print(\"\\nMissing values handled for categorical features.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Train/Test split\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain/Test split complete.\")\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Column Transformer (One-Hot for categorical)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ Logistic Regression Model\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8️⃣ Fit model\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nModel training complete.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9️⃣ Predictions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(roc_auc_score(y_test, y_proba))\n",
    "\n",
    "print(\"\\nPIPELINE COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec031ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/usr/bin/python3\n",
      "3.9.6 (default, Apr 30 2025, 02:07:18) \n",
      "[Clang 17.0.0 (clang-1700.0.13.5)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77d070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a6debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cleaned dataset: (9240, 31)\n",
      "Model dataset shape: (9240, 25)\n",
      "Columns ready for modeling.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Load cleaned dataset\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df_clean = pd.read_csv(\"../data/processed/lead_scoring_cleaned.csv\")\n",
    "\n",
    "print(\"Loaded cleaned dataset:\", df_clean.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Remove leakage + ID columns\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "DROP_COLUMNS = [\n",
    "    \"Prospect ID\",\n",
    "    \"Lead Number\",\n",
    "    \"Lead Quality\",\n",
    "    \"Tags\",\n",
    "    \"Lead Profile\",\n",
    "    \"Last Notable Activity\"\n",
    "]\n",
    "\n",
    "existing_drop = [col for col in DROP_COLUMNS if col in df_clean.columns]\n",
    "\n",
    "df_model = df_clean.drop(columns=existing_drop).copy()\n",
    "\n",
    "print(\"Model dataset shape:\", df_model.shape)\n",
    "print(\"Columns ready for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e159fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "LOGISTIC REGRESSION PIPELINE START\n",
      "====================================================================================================\n",
      "\n",
      "Categorical columns: 21\n",
      "['Lead Origin', 'Lead Source', 'Do Not Email', 'Do Not Call', 'Last Activity', 'Country', 'Specialization', 'How did you hear about X Education', 'What is your current occupation', 'What matters most to you in choosing a course', 'Search', 'Magazine', 'Newspaper Article', 'X Education Forums', 'Newspaper', 'Digital Advertisement', 'Through Recommendations', 'Receive More Updates About Our Courses', 'Update me on Supply Chain Content', 'Get updates on DM Content', 'City']\n",
      "\n",
      "Numeric columns: 3\n",
      "['TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit']\n",
      "\n",
      "Missing values handled for categorical features.\n",
      "\n",
      "Train/Test split complete.\n",
      "Train size: (7392, 24)\n",
      "Test size: (1848, 24)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 88\u001b[0m\n\u001b[1;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     80\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),\n\u001b[1;32m     81\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m     82\u001b[0m ])\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# 8️⃣ Fit model\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel training complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 9️⃣ Predictions\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1222\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1222\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1014\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[1;32m   1013\u001b[0m     _ensure_no_complex_data(array)\n\u001b[0;32m-> 1014\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D input, got input with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1028\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1029\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:649\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(sparse_container, accept_sparse, dtype, copy, ensure_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    644\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt check \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparse_container\u001b[38;5;241m.\u001b[39mformat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sparse matrix for nan or inf.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    646\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    647\u001b[0m         )\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 649\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m            \u001b[49m\u001b[43msparse_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# TODO: Remove when the minimum version of SciPy supported is 1.12\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# With SciPy sparse arrays, conversion from DIA format to COO, CSR, or BSR\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# triggers the use of `np.int64` indices even if the data is such that it could\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# algorithms support large indices, the following code downcasts to `np.int32`\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# indices when it's safe to do so.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m changed_format:\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;66;03m# accept_sparse is specified to a specific format and a conversion occurred\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"LOGISTIC REGRESSION PIPELINE START\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Copy dataset\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df = df_model.copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Separate target\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "y = df[\"Converted\"]\n",
    "X = df.drop(columns=[\"Converted\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Identify column types\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nCategorical columns:\", len(categorical_cols))\n",
    "print(categorical_cols)\n",
    "\n",
    "print(\"\\nNumeric columns:\", len(numeric_cols))\n",
    "print(numeric_cols)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Fill missing for categorical columns only\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].fillna(\"Missing\")\n",
    "\n",
    "print(\"\\nMissing values handled for categorical features.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Train/Test split\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain/Test split complete.\")\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Column Transformer (One-Hot for categorical)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", \"passthrough\", numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ Logistic Regression Model\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8️⃣ Fit model\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nModel training complete.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9️⃣ Predictions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(roc_auc_score(y_test, y_proba))\n",
    "\n",
    "print(\"\\nPIPELINE COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54de100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "LOGISTIC REGRESSION PIPELINE START (with imputers)\n",
      "====================================================================================================\n",
      "\n",
      "Categorical columns: 21\n",
      "['Lead Origin', 'Lead Source', 'Do Not Email', 'Do Not Call', 'Last Activity', 'Country', 'Specialization', 'How did you hear about X Education', 'What is your current occupation', 'What matters most to you in choosing a course', 'Search', 'Magazine', 'Newspaper Article', 'X Education Forums', 'Newspaper', 'Digital Advertisement', 'Through Recommendations', 'Receive More Updates About Our Courses', 'Update me on Supply Chain Content', 'Get updates on DM Content', 'City']\n",
      "\n",
      "Numeric columns: 3\n",
      "['TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit']\n",
      "\n",
      "NaNs in numeric cols BEFORE pipeline:\n",
      "TotalVisits                    137\n",
      "Total Time Spent on Website      0\n",
      "Page Views Per Visit           137\n",
      "dtype: int64\n",
      "\n",
      "Train/Test split complete.\n",
      "Train size: (7392, 24)\n",
      "Test size: (1848, 24)\n",
      "\n",
      "Model training complete.\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84      1136\n",
      "           1       0.76      0.70      0.73       712\n",
      "\n",
      "    accuracy                           0.80      1848\n",
      "   macro avg       0.79      0.78      0.78      1848\n",
      "weighted avg       0.80      0.80      0.80      1848\n",
      "\n",
      "\n",
      "ROC-AUC Score:\n",
      "0.8751\n",
      "\n",
      "PIPELINE COMPLETE\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dancingfoxstudio/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"LOGISTIC REGRESSION PIPELINE START (with imputers)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Copy dataset\n",
    "# ------------------------------------------------------------\n",
    "df = df_model.copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Separate target\n",
    "# ------------------------------------------------------------\n",
    "y = df[\"Converted\"]\n",
    "X = df.drop(columns=[\"Converted\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Identify column types\n",
    "# ------------------------------------------------------------\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nCategorical columns:\", len(categorical_cols))\n",
    "print(categorical_cols)\n",
    "\n",
    "print(\"\\nNumeric columns:\", len(numeric_cols))\n",
    "print(numeric_cols)\n",
    "\n",
    "# Quick NaN check before pipeline (informational)\n",
    "print(\"\\nNaNs in numeric cols BEFORE pipeline:\")\n",
    "print(X[numeric_cols].isna().sum())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Train/Test split\n",
    "# ------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain/Test split complete.\")\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Preprocessing pipelines\n",
    "# ------------------------------------------------------------\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", cat_pipeline, categorical_cols),\n",
    "        (\"num\", num_pipeline, numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ Logistic Regression Model\n",
    "# ------------------------------------------------------------\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ Fit\n",
    "# ------------------------------------------------------------\n",
    "model.fit(X_train, y_train)\n",
    "print(\"\\nModel training complete.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8️⃣ Predict + Evaluate\n",
    "# ------------------------------------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(round(roc_auc_score(y_test, y_proba), 4))\n",
    "\n",
    "print(\"\\nPIPELINE COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5d0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "MODEL INTERPRETATION: TOP COEFFICIENTS + ODDS RATIOS\n",
      "====================================================================================================\n",
      "\n",
      "Intercept (log-odds): -0.218\n",
      "Intercept (odds): 0.8041\n",
      "\n",
      "TOP FEATURES BY ABSOLUTE EFFECT (|coef|)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef_logit</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lead Source_Welingak Website</td>\n",
       "      <td>1.968850</td>\n",
       "      <td>7.162434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Origin_Lead Add Form</td>\n",
       "      <td>1.695148</td>\n",
       "      <td>5.447453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Country_Missing</td>\n",
       "      <td>1.649572</td>\n",
       "      <td>5.204750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>What is your current occupation_Working Profes...</td>\n",
       "      <td>1.577276</td>\n",
       "      <td>4.841747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Last Activity_SMS Sent</td>\n",
       "      <td>1.446735</td>\n",
       "      <td>4.249217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Last Activity_Had a Phone Conversation</td>\n",
       "      <td>1.348164</td>\n",
       "      <td>3.850348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Last Activity_Olark Chat Conversation</td>\n",
       "      <td>-1.269479</td>\n",
       "      <td>0.280978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lead Origin_Landing Page Submission</td>\n",
       "      <td>-1.054339</td>\n",
       "      <td>0.348423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Last Activity_Unsubscribed</td>\n",
       "      <td>0.971903</td>\n",
       "      <td>2.642968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Last Activity_Converted to Lead</td>\n",
       "      <td>-0.965445</td>\n",
       "      <td>0.380814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lead Source_Facebook</td>\n",
       "      <td>-0.962947</td>\n",
       "      <td>0.381766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lead Origin_Lead Import</td>\n",
       "      <td>-0.962521</td>\n",
       "      <td>0.381929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>What is your current occupation_Unemployed</td>\n",
       "      <td>-0.897208</td>\n",
       "      <td>0.407706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>What is your current occupation_Student</td>\n",
       "      <td>-0.819574</td>\n",
       "      <td>0.440619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Last Activity_Missing</td>\n",
       "      <td>-0.808177</td>\n",
       "      <td>0.445670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Do Not Email_Yes</td>\n",
       "      <td>-0.784683</td>\n",
       "      <td>0.456264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>What matters most to you in choosing a course_...</td>\n",
       "      <td>-0.778979</td>\n",
       "      <td>0.458874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>What matters most to you in choosing a course_...</td>\n",
       "      <td>0.766066</td>\n",
       "      <td>2.151285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>What is your current occupation_Housewife</td>\n",
       "      <td>0.756433</td>\n",
       "      <td>2.130662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Last Activity_Email Bounced</td>\n",
       "      <td>-0.699606</td>\n",
       "      <td>0.496781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Specialization_Hospitality Management</td>\n",
       "      <td>-0.695497</td>\n",
       "      <td>0.498826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Specialization_Missing</td>\n",
       "      <td>-0.681593</td>\n",
       "      <td>0.505810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Last Activity_Form Submitted on Website</td>\n",
       "      <td>-0.668743</td>\n",
       "      <td>0.512352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Do Not Email_No</td>\n",
       "      <td>0.623653</td>\n",
       "      <td>1.865731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>City_Missing</td>\n",
       "      <td>-0.611608</td>\n",
       "      <td>0.542478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               feature  coef_logit  odds_ratio\n",
       "20                        Lead Source_Welingak Website    1.968850    7.162434\n",
       "2                            Lead Origin_Lead Add Form    1.695148    5.447453\n",
       "60                                     Country_Missing    1.649572    5.204750\n",
       "113  What is your current occupation_Working Profes...    1.577276    4.841747\n",
       "37                              Last Activity_SMS Sent    1.446735    4.249217\n",
       "32              Last Activity_Had a Phone Conversation    1.348164    3.850348\n",
       "34               Last Activity_Olark Chat Conversation   -1.269479    0.280978\n",
       "1                  Lead Origin_Landing Page Submission   -1.054339    0.348423\n",
       "39                          Last Activity_Unsubscribed    0.971903    2.642968\n",
       "26                     Last Activity_Converted to Lead   -0.965445    0.380814\n",
       "8                                 Lead Source_Facebook   -0.962947    0.381766\n",
       "3                              Lead Origin_Lead Import   -0.962521    0.381929\n",
       "112         What is your current occupation_Unemployed   -0.897208    0.407706\n",
       "111            What is your current occupation_Student   -0.819574    0.440619\n",
       "33                               Last Activity_Missing   -0.808177    0.445670\n",
       "22                                    Do Not Email_Yes   -0.784683    0.456264\n",
       "116  What matters most to you in choosing a course_...   -0.778979    0.458874\n",
       "114  What matters most to you in choosing a course_...    0.766066    2.151285\n",
       "108          What is your current occupation_Housewife    0.756433    2.130662\n",
       "27                         Last Activity_Email Bounced   -0.699606    0.496781\n",
       "84               Specialization_Hospitality Management   -0.695497    0.498826\n",
       "90                              Specialization_Missing   -0.681593    0.505810\n",
       "31             Last Activity_Form Submitted on Website   -0.668743    0.512352\n",
       "21                                     Do Not Email_No    0.623653    1.865731\n",
       "133                                       City_Missing   -0.611608    0.542478"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP POSITIVE DRIVERS (increase conversion odds)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef_logit</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lead Source_Welingak Website</td>\n",
       "      <td>1.968850</td>\n",
       "      <td>7.162434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Origin_Lead Add Form</td>\n",
       "      <td>1.695148</td>\n",
       "      <td>5.447453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Country_Missing</td>\n",
       "      <td>1.649572</td>\n",
       "      <td>5.204750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>What is your current occupation_Working Profes...</td>\n",
       "      <td>1.577276</td>\n",
       "      <td>4.841747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Last Activity_SMS Sent</td>\n",
       "      <td>1.446735</td>\n",
       "      <td>4.249217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Last Activity_Had a Phone Conversation</td>\n",
       "      <td>1.348164</td>\n",
       "      <td>3.850348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Last Activity_Unsubscribed</td>\n",
       "      <td>0.971903</td>\n",
       "      <td>2.642968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>What matters most to you in choosing a course_...</td>\n",
       "      <td>0.766066</td>\n",
       "      <td>2.151285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>What is your current occupation_Housewife</td>\n",
       "      <td>0.756433</td>\n",
       "      <td>2.130662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Do Not Email_No</td>\n",
       "      <td>0.623653</td>\n",
       "      <td>1.865731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Last Activity_Approached upfront</td>\n",
       "      <td>0.598975</td>\n",
       "      <td>1.820252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Country_United Arab Emirates</td>\n",
       "      <td>0.584296</td>\n",
       "      <td>1.793727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>How did you hear about X Education_Email</td>\n",
       "      <td>0.565137</td>\n",
       "      <td>1.759689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>How did you hear about X Education_Advertisements</td>\n",
       "      <td>0.327661</td>\n",
       "      <td>1.387719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Last Activity_Resubscribed to emails</td>\n",
       "      <td>0.324823</td>\n",
       "      <td>1.383786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Specialization_Banking, Investment And Insurance</td>\n",
       "      <td>0.293931</td>\n",
       "      <td>1.341691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Specialization_E-COMMERCE</td>\n",
       "      <td>0.271330</td>\n",
       "      <td>1.311707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Specialization_Finance Management</td>\n",
       "      <td>0.225085</td>\n",
       "      <td>1.252429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Country_United Kingdom</td>\n",
       "      <td>0.218636</td>\n",
       "      <td>1.244378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Country_Oman</td>\n",
       "      <td>0.211413</td>\n",
       "      <td>1.235423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Country_India</td>\n",
       "      <td>0.197645</td>\n",
       "      <td>1.218529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Last Activity_Email Opened</td>\n",
       "      <td>0.193035</td>\n",
       "      <td>1.212925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>City_Tier II Cities</td>\n",
       "      <td>0.186924</td>\n",
       "      <td>1.205536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Specialization_Marketing Management</td>\n",
       "      <td>0.183297</td>\n",
       "      <td>1.201171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Specialization_Healthcare Management</td>\n",
       "      <td>0.161098</td>\n",
       "      <td>1.174800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               feature  coef_logit  odds_ratio\n",
       "20                        Lead Source_Welingak Website    1.968850    7.162434\n",
       "2                            Lead Origin_Lead Add Form    1.695148    5.447453\n",
       "60                                     Country_Missing    1.649572    5.204750\n",
       "113  What is your current occupation_Working Profes...    1.577276    4.841747\n",
       "37                              Last Activity_SMS Sent    1.446735    4.249217\n",
       "32              Last Activity_Had a Phone Conversation    1.348164    3.850348\n",
       "39                          Last Activity_Unsubscribed    0.971903    2.642968\n",
       "114  What matters most to you in choosing a course_...    0.766066    2.151285\n",
       "108          What is your current occupation_Housewife    0.756433    2.130662\n",
       "21                                     Do Not Email_No    0.623653    1.865731\n",
       "25                    Last Activity_Approached upfront    0.598975    1.820252\n",
       "74                        Country_United Arab Emirates    0.584296    1.793727\n",
       "98            How did you hear about X Education_Email    0.565137    1.759689\n",
       "97   How did you hear about X Education_Advertisements    0.327661    1.387719\n",
       "36                Last Activity_Resubscribed to emails    0.324823    1.383786\n",
       "78    Specialization_Banking, Investment And Insurance    0.293931    1.341691\n",
       "81                           Specialization_E-COMMERCE    0.271330    1.311707\n",
       "82                   Specialization_Finance Management    0.225085    1.252429\n",
       "75                              Country_United Kingdom    0.218636    1.244378\n",
       "63                                        Country_Oman    0.211413    1.235423\n",
       "54                                       Country_India    0.197645    1.218529\n",
       "29                          Last Activity_Email Opened    0.193035    1.212925\n",
       "139                                City_Tier II Cities    0.186924    1.205536\n",
       "88                 Specialization_Marketing Management    0.183297    1.201171\n",
       "83                Specialization_Healthcare Management    0.161098    1.174800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP NEGATIVE DRIVERS (decrease conversion odds)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef_logit</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Last Activity_Olark Chat Conversation</td>\n",
       "      <td>-1.269479</td>\n",
       "      <td>0.280978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lead Origin_Landing Page Submission</td>\n",
       "      <td>-1.054339</td>\n",
       "      <td>0.348423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Last Activity_Converted to Lead</td>\n",
       "      <td>-0.965445</td>\n",
       "      <td>0.380814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lead Source_Facebook</td>\n",
       "      <td>-0.962947</td>\n",
       "      <td>0.381766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lead Origin_Lead Import</td>\n",
       "      <td>-0.962521</td>\n",
       "      <td>0.381929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>What is your current occupation_Unemployed</td>\n",
       "      <td>-0.897208</td>\n",
       "      <td>0.407706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>What is your current occupation_Student</td>\n",
       "      <td>-0.819574</td>\n",
       "      <td>0.440619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Last Activity_Missing</td>\n",
       "      <td>-0.808177</td>\n",
       "      <td>0.445670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Do Not Email_Yes</td>\n",
       "      <td>-0.784683</td>\n",
       "      <td>0.456264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>What matters most to you in choosing a course_...</td>\n",
       "      <td>-0.778979</td>\n",
       "      <td>0.458874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Last Activity_Email Bounced</td>\n",
       "      <td>-0.699606</td>\n",
       "      <td>0.496781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Specialization_Hospitality Management</td>\n",
       "      <td>-0.695497</td>\n",
       "      <td>0.498826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Specialization_Missing</td>\n",
       "      <td>-0.681593</td>\n",
       "      <td>0.505810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Last Activity_Form Submitted on Website</td>\n",
       "      <td>-0.668743</td>\n",
       "      <td>0.512352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>City_Missing</td>\n",
       "      <td>-0.611608</td>\n",
       "      <td>0.542478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Country_Saudi Arabia</td>\n",
       "      <td>-0.579515</td>\n",
       "      <td>0.560170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>What is your current occupation_Missing</td>\n",
       "      <td>-0.575717</td>\n",
       "      <td>0.562302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>How did you hear about X Education_Multiple So...</td>\n",
       "      <td>-0.527576</td>\n",
       "      <td>0.590034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>How did you hear about X Education_SMS</td>\n",
       "      <td>-0.515710</td>\n",
       "      <td>0.597076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Country_Australia</td>\n",
       "      <td>-0.411348</td>\n",
       "      <td>0.662757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lead Source_Olark Chat</td>\n",
       "      <td>-0.406715</td>\n",
       "      <td>0.665834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Last Activity_Page Visited on Website</td>\n",
       "      <td>-0.385502</td>\n",
       "      <td>0.680109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lead Source_Direct Traffic</td>\n",
       "      <td>-0.351088</td>\n",
       "      <td>0.703922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Country_Italy</td>\n",
       "      <td>-0.327295</td>\n",
       "      <td>0.720871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Country_Netherlands</td>\n",
       "      <td>-0.312262</td>\n",
       "      <td>0.731790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               feature  coef_logit  odds_ratio\n",
       "34               Last Activity_Olark Chat Conversation   -1.269479    0.280978\n",
       "1                  Lead Origin_Landing Page Submission   -1.054339    0.348423\n",
       "26                     Last Activity_Converted to Lead   -0.965445    0.380814\n",
       "8                                 Lead Source_Facebook   -0.962947    0.381766\n",
       "3                              Lead Origin_Lead Import   -0.962521    0.381929\n",
       "112         What is your current occupation_Unemployed   -0.897208    0.407706\n",
       "111            What is your current occupation_Student   -0.819574    0.440619\n",
       "33                               Last Activity_Missing   -0.808177    0.445670\n",
       "22                                    Do Not Email_Yes   -0.784683    0.456264\n",
       "116  What matters most to you in choosing a course_...   -0.778979    0.458874\n",
       "27                         Last Activity_Email Bounced   -0.699606    0.496781\n",
       "84               Specialization_Hospitality Management   -0.695497    0.498826\n",
       "90                              Specialization_Missing   -0.681593    0.505810\n",
       "31             Last Activity_Form Submitted on Website   -0.668743    0.512352\n",
       "133                                       City_Missing   -0.611608    0.542478\n",
       "67                                Country_Saudi Arabia   -0.579515    0.560170\n",
       "109            What is your current occupation_Missing   -0.575717    0.562302\n",
       "100  How did you hear about X Education_Multiple So...   -0.527576    0.590034\n",
       "103             How did you hear about X Education_SMS   -0.515710    0.597076\n",
       "43                                   Country_Australia   -0.411348    0.662757\n",
       "12                              Lead Source_Olark Chat   -0.406715    0.665834\n",
       "35               Last Activity_Page Visited on Website   -0.385502    0.680109\n",
       "7                           Lead Source_Direct Traffic   -0.351088    0.703922\n",
       "56                                       Country_Italy   -0.327295    0.720871\n",
       "61                                 Country_Netherlands   -0.312262    0.731790"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NUMERIC FEATURES (odds ratios per +1 unit)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'abs_coef'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hm/vcz2lsg157v_xn27jmdy2ds40000gn/T/ipykernel_77231/1492956533.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7192\u001b[0m             )\n\u001b[1;32m   7193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7194\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7196\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7198\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abs_coef'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"MODEL INTERPRETATION: TOP COEFFICIENTS + ODDS RATIOS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1) Extract components\n",
    "pre = model.named_steps[\"preprocessing\"]\n",
    "clf = model.named_steps[\"classifier\"]\n",
    "\n",
    "# 2) Get feature names from ColumnTransformer\n",
    "cat_pipe = pre.named_transformers_[\"cat\"]\n",
    "ohe = cat_pipe.named_steps[\"ohe\"]\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "\n",
    "num_feature_names = np.array(numeric_cols, dtype=object)\n",
    "feature_names = np.concatenate([cat_feature_names, num_feature_names])\n",
    "\n",
    "# 3) Pull coefficients\n",
    "coefs = clf.coef_.ravel()\n",
    "intercept = float(clf.intercept_[0])\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coef_logit\": coefs,\n",
    "    \"odds_ratio\": np.exp(coefs),\n",
    "    \"abs_coef\": np.abs(coefs),\n",
    "}).sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "print(\"\\nIntercept (log-odds):\", round(intercept, 4))\n",
    "print(\"Intercept (odds):\", round(np.exp(intercept), 4))\n",
    "\n",
    "# 4) Show top drivers by absolute effect\n",
    "TOP_N = 25\n",
    "\n",
    "print(\"\\nTOP FEATURES BY ABSOLUTE EFFECT (|coef|)\")\n",
    "print(\"-\"*80)\n",
    "display(coef_df.head(TOP_N)[[\"feature\", \"coef_logit\", \"odds_ratio\"]])\n",
    "\n",
    "# 5) Show strongest positive and negative\n",
    "print(\"\\nTOP POSITIVE DRIVERS (increase conversion odds)\")\n",
    "print(\"-\"*80)\n",
    "display(coef_df.sort_values(\"coef_logit\", ascending=False).head(TOP_N)[[\"feature\", \"coef_logit\", \"odds_ratio\"]])\n",
    "\n",
    "print(\"\\nTOP NEGATIVE DRIVERS (decrease conversion odds)\")\n",
    "print(\"-\"*80)\n",
    "display(coef_df.sort_values(\"coef_logit\", ascending=True).head(TOP_N)[[\"feature\", \"coef_logit\", \"odds_ratio\"]])\n",
    "\n",
    "# 6) Numeric-only effects\n",
    "print(\"\\nNUMERIC FEATURES (odds ratios per +1 unit)\")\n",
    "print(\"-\"*80)\n",
    "num_df = coef_df[coef_df[\"feature\"].isin(numeric_cols)].copy()\n",
    "display(num_df[[\"feature\", \"coef_logit\", \"odds_ratio\"]].sort_values(\"abs_coef\", ascending=False))\n",
    "\n",
    "print(\"\\nINTERPRETATION BLOCK COMPLETE\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb05d621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NUMERIC FEATURES (odds ratios per +1 unit)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coef_logit</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Page Views Per Visit</td>\n",
       "      <td>-0.096891</td>\n",
       "      <td>0.907655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>TotalVisits</td>\n",
       "      <td>0.040456</td>\n",
       "      <td>1.041286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Total Time Spent on Website</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>1.002050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         feature  coef_logit  odds_ratio\n",
       "142         Page Views Per Visit   -0.096891    0.907655\n",
       "140                  TotalVisits    0.040456    1.041286\n",
       "141  Total Time Spent on Website    0.002048    1.002050"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nNUMERIC FEATURES (odds ratios per +1 unit)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "num_df = coef_df[coef_df[\"feature\"].isin(numeric_cols)].copy()\n",
    "num_df[\"abs_coef\"] = num_df[\"coef_logit\"].abs()   # <- robust, egal was vorher war\n",
    "\n",
    "display(num_df.sort_values(\"abs_coef\", ascending=False)[[\"feature\", \"coef_logit\", \"odds_ratio\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e3bbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC full model: 0.8596\n",
      "AUC without Last Activity: 0.838\n",
      "ΔAUC (drop): 0.0216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def train_logit_auc(df_in, drop_cols=None):\n",
    "    df = df_in.copy()\n",
    "    drop_cols = drop_cols or []\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    y = df[\"Converted\"]\n",
    "    X = df.drop(columns=[\"Converted\"])\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "    num_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_pipeline, categorical_cols),\n",
    "            (\"num\", num_pipeline, numeric_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(max_iter=10000, solver=\"saga\"))\n",
    "    ])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    return auc\n",
    "\n",
    "auc_full = train_logit_auc(df_model, drop_cols=[])\n",
    "auc_no_last_activity = train_logit_auc(df_model, drop_cols=[\"Last Activity\"])\n",
    "\n",
    "print(\"AUC full model:\", round(auc_full, 4))\n",
    "print(\"AUC without Last Activity:\", round(auc_no_last_activity, 4))\n",
    "print(\"ΔAUC (drop):\", round(auc_full - auc_no_last_activity, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "152fde1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "STRUCTURAL / CAUSAL MODEL\n",
      "====================================================================================================\n",
      "\n",
      "Categorical: 20\n",
      "Numeric: 3\n",
      "\n",
      "AUC: 0.827\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      1136\n",
      "           1       0.75      0.64      0.69       712\n",
      "\n",
      "    accuracy                           0.78      1848\n",
      "   macro avg       0.77      0.75      0.76      1848\n",
      "weighted avg       0.78      0.78      0.77      1848\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"STRUCTURAL / CAUSAL MODEL\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "df = df_model.copy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ Remove process / funnel proximity features\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "DROP_COLS = [\n",
    "    \"Last Activity\",\n",
    "    \"Last Notable Activity\",\n",
    "    \"Tags\",\n",
    "    \"Lead Quality\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in DROP_COLS if c in df.columns])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ Separate target\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "y = df[\"Converted\"]\n",
    "X = df.drop(columns=[\"Converted\"])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ Identify types\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nCategorical:\", len(categorical_cols))\n",
    "print(\"Numeric:\", len(numeric_cols))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ Pipelines\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # <- key change\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", cat_pipeline, categorical_cols),\n",
    "        (\"num\", num_pipeline, numeric_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ Train/Test\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\nAUC:\", round(roc_auc_score(y_test, y_proba), 4))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddd78044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "STRUCTURAL BASE DATASET\n",
      "====================================================================================================\n",
      "Shape: (9240, 24)\n",
      "\n",
      "====================================================================================================\n",
      "FEATURE DROP TEST: GEO\n",
      "====================================================================================================\n",
      "Structural FULL AUC:              0.8270\n",
      "Structural without Country AUC:   0.8274   (Δ -0.0005)\n",
      "Structural without Country+City:  0.8294   (Δ -0.0024)\n",
      "\n",
      "====================================================================================================\n",
      "BLOCK CONTRIBUTION ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "Blocks (available columns):\n",
      "- A_Marketing: ['Lead Origin', 'Lead Source']\n",
      "- B_Profile: ['What is your current occupation', 'Specialization']\n",
      "- C_Behavior: ['TotalVisits', 'Total Time Spent on Website', 'Page Views Per Visit']\n",
      "- D_Geo: ['Country', 'City']\n",
      "\n",
      "--- AUC per Block (ALONE) ---\n",
      "  C_Behavior: AUC=0.7214 | n_features=3\n",
      " A_Marketing: AUC=0.6346 | n_features=2\n",
      "   B_Profile: AUC=0.6103 | n_features=2\n",
      "       D_Geo: AUC=0.5154 | n_features=2\n",
      "\n",
      "--- Incremental AUC (CUMULATIVE) ---\n",
      "Up to  A_Marketing: AUC=0.6346  | total_features=2\n",
      "Up to    B_Profile: AUC=0.6997 (+0.0652) | total_features=4\n",
      "Up to   C_Behavior: AUC=0.8273 (+0.1276) | total_features=7\n",
      "Up to        D_Geo: AUC=0.8244 (+-0.0030) | total_features=9\n",
      "\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ====================================================================================\n",
    "# CONFIG\n",
    "# ====================================================================================\n",
    "\n",
    "TARGET = \"Converted\"\n",
    "\n",
    "# Features we consider \"process / funnel proximity\" (excluded for structural/causal view)\n",
    "PROCESS_COLS = [\"Last Activity\", \"Last Notable Activity\", \"Tags\", \"Lead Quality\"]\n",
    "\n",
    "# Structural blocks\n",
    "BLOCKS = {\n",
    "    \"A_Marketing\": [\"Lead Origin\", \"Lead Source\"],\n",
    "    \"B_Profile\": [\"What is your current occupation\", \"Specialization\"],\n",
    "    \"C_Behavior\": [\"TotalVisits\", \"Total Time Spent on Website\", \"Page Views Per Visit\"],\n",
    "    \"D_Geo\": [\"Country\", \"City\"],\n",
    "}\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "MAX_ITER = 5000\n",
    "\n",
    "# ====================================================================================\n",
    "# HELPERS\n",
    "# ====================================================================================\n",
    "\n",
    "def build_structural_df(df_model: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Start from df_model and remove process/proximity variables (structural view).\"\"\"\n",
    "    df = df_model.copy()\n",
    "    df = df.drop(columns=[c for c in PROCESS_COLS if c in df.columns])\n",
    "    # Keep target\n",
    "    assert TARGET in df.columns, f\"Target '{TARGET}' not found.\"\n",
    "    return df\n",
    "\n",
    "def fit_logit_auc(df_in: pd.DataFrame, feature_cols: list[str]) -> float:\n",
    "    \"\"\"Fit a logistic regression with proper preprocessing and return test AUC.\"\"\"\n",
    "    # Select X/y\n",
    "    X = df_in[feature_cols].copy()\n",
    "    y = df_in[TARGET].copy()\n",
    "\n",
    "    # Identify types\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    # Pipelines\n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # structural: neutralize missingness\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    num_pipeline = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_pipeline, categorical_cols),\n",
    "            (\"num\", num_pipeline, numeric_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(max_iter=MAX_ITER))\n",
    "    ])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    return auc\n",
    "\n",
    "def available_cols(df: pd.DataFrame, cols: list[str]) -> list[str]:\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "# ====================================================================================\n",
    "# 1) PREPARE STRUCTURAL DATAFRAME\n",
    "# ====================================================================================\n",
    "\n",
    "df_struct = build_structural_df(df_model)\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"STRUCTURAL BASE DATASET\")\n",
    "print(\"=\"*100)\n",
    "print(\"Shape:\", df_struct.shape)\n",
    "\n",
    "all_feature_cols = [c for c in df_struct.columns if c != TARGET]\n",
    "\n",
    "# ====================================================================================\n",
    "# 2) FEATURE DROP TEST: Country / City\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FEATURE DROP TEST: GEO\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Structural Full (all features in df_struct)\n",
    "auc_struct_full = fit_logit_auc(df_struct, feature_cols=all_feature_cols)\n",
    "\n",
    "# Structural without Country\n",
    "cols_no_country = [c for c in all_feature_cols if c != \"Country\"]\n",
    "auc_no_country = fit_logit_auc(df_struct, feature_cols=cols_no_country)\n",
    "\n",
    "# Structural without Country + City\n",
    "cols_no_country_city = [c for c in all_feature_cols if c not in [\"Country\", \"City\"]]\n",
    "auc_no_country_city = fit_logit_auc(df_struct, feature_cols=cols_no_country_city)\n",
    "\n",
    "print(f\"Structural FULL AUC:              {auc_struct_full:.4f}\")\n",
    "print(f\"Structural without Country AUC:   {auc_no_country:.4f}   (Δ {auc_struct_full-auc_no_country:+.4f})\")\n",
    "print(f\"Structural without Country+City:  {auc_no_country_city:.4f}   (Δ {auc_struct_full-auc_no_country_city:+.4f})\")\n",
    "\n",
    "# ====================================================================================\n",
    "# 3) BLOCK CONTRIBUTION ANALYSIS\n",
    "# ====================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BLOCK CONTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Ensure blocks only contain available columns\n",
    "blocks_clean = {k: available_cols(df_struct, v) for k, v in BLOCKS.items()}\n",
    "\n",
    "print(\"\\nBlocks (available columns):\")\n",
    "for k, v in blocks_clean.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "# A) AUC using each block alone\n",
    "print(\"\\n--- AUC per Block (ALONE) ---\")\n",
    "block_alone_results = []\n",
    "for block_name, cols in blocks_clean.items():\n",
    "    if len(cols) == 0:\n",
    "        continue\n",
    "    auc = fit_logit_auc(df_struct, feature_cols=cols)\n",
    "    block_alone_results.append((block_name, auc, cols))\n",
    "\n",
    "block_alone_results = sorted(block_alone_results, key=lambda x: x[1], reverse=True)\n",
    "for name, auc, cols in block_alone_results:\n",
    "    print(f\"{name:>12}: AUC={auc:.4f} | n_features={len(cols)}\")\n",
    "\n",
    "# B) Incremental build-up: A -> A+B -> A+B+C -> A+B+C+D\n",
    "print(\"\\n--- Incremental AUC (CUMULATIVE) ---\")\n",
    "order = [\"A_Marketing\", \"B_Profile\", \"C_Behavior\", \"D_Geo\"]\n",
    "\n",
    "cum_cols = []\n",
    "cum_results = []\n",
    "for block_name in order:\n",
    "    cols = blocks_clean.get(block_name, [])\n",
    "    cum_cols += cols\n",
    "    if len(cum_cols) == 0:\n",
    "        continue\n",
    "    auc = fit_logit_auc(df_struct, feature_cols=cum_cols)\n",
    "    cum_results.append((block_name, auc, len(cum_cols)))\n",
    "\n",
    "prev_auc = None\n",
    "for block_name, auc, ncols in cum_results:\n",
    "    delta = \"\" if prev_auc is None else f\"(+{auc-prev_auc:.4f})\"\n",
    "    print(f\"Up to {block_name:>12}: AUC={auc:.4f} {delta} | total_features={ncols}\")\n",
    "    prev_auc = auc\n",
    "\n",
    "print(\"\\nDONE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
